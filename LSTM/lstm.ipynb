{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Problem 1] Execution of various methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-31 12:14:55.593997: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-07-31 12:14:56.869536: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m114s\u001b[0m 142ms/step - accuracy: 0.7150 - loss: 0.5418 - val_accuracy: 0.8344 - val_loss: 0.3789\n",
      "Epoch 2/5\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m98s\u001b[0m 125ms/step - accuracy: 0.8762 - loss: 0.3046 - val_accuracy: 0.8300 - val_loss: 0.3847\n",
      "Epoch 3/5\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m99s\u001b[0m 126ms/step - accuracy: 0.9131 - loss: 0.2238 - val_accuracy: 0.8330 - val_loss: 0.3956\n",
      "Epoch 4/5\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m100s\u001b[0m 127ms/step - accuracy: 0.9433 - loss: 0.1585 - val_accuracy: 0.8300 - val_loss: 0.4672\n",
      "Epoch 5/5\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m96s\u001b[0m 123ms/step - accuracy: 0.9598 - loss: 0.1099 - val_accuracy: 0.8210 - val_loss: 0.5174\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 25ms/step - accuracy: 0.8192 - loss: 0.5264\n",
      "Test score: 0.5173606276512146\n",
      "Test accuracy: 0.8209999799728394\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
    "from tensorflow.keras.datasets import imdb\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "\n",
    "# Parameters\n",
    "max_features = 20000\n",
    "maxlen = 80\n",
    "batch_size = 32\n",
    "epochs = 5\n",
    "\n",
    "# Load data\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)\n",
    "\n",
    "# Pad sequences\n",
    "x_train = sequence.pad_sequences(x_train, maxlen=maxlen)\n",
    "x_test = sequence.pad_sequences(x_test, maxlen=maxlen)\n",
    "\n",
    "# Model\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_features, 128))\n",
    "model.add(LSTM(128, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# Compile model\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Train model\n",
    "model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, validation_data=(x_test, y_test))\n",
    "\n",
    "# Evaluate model\n",
    "score, acc = model.evaluate(x_test, y_test, batch_size=batch_size)\n",
    "print('Test score:', score)\n",
    "print('Test accuracy:', acc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m88s\u001b[0m 109ms/step - accuracy: 0.6380 - loss: 1.2565 - val_accuracy: 0.6790 - val_loss: 0.5877\n",
      "Epoch 2/5\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m80s\u001b[0m 102ms/step - accuracy: 0.7665 - loss: 0.4925 - val_accuracy: 0.6972 - val_loss: 0.5722\n",
      "Epoch 3/5\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m85s\u001b[0m 108ms/step - accuracy: 0.8295 - loss: 122.9253 - val_accuracy: 0.6336 - val_loss: 0.6432\n",
      "Epoch 4/5\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m84s\u001b[0m 107ms/step - accuracy: 0.7498 - loss: 0.5372 - val_accuracy: 0.6578 - val_loss: 0.6302\n",
      "Epoch 5/5\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 106ms/step - accuracy: 0.8183 - loss: 0.4102 - val_accuracy: 0.6632 - val_loss: 0.6336\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 18ms/step - accuracy: 0.6592 - loss: 0.6407\n",
      "Test score: 0.6336373686790466\n",
      "Test accuracy: 0.6632000207901001\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers import GRU\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_features, 128))\n",
    "model.add(GRU(128, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, validation_data=(x_test, y_test))\n",
    "\n",
    "score, acc = model.evaluate(x_test, y_test, batch_size=batch_size)\n",
    "print('Test score:', score)\n",
    "print('Test accuracy:', acc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SimpleRNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 59ms/step - accuracy: 0.5109 - loss: 0.7057 - val_accuracy: 0.6059 - val_loss: 0.6659\n",
      "Epoch 2/5\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 58ms/step - accuracy: 0.6433 - loss: 0.6268 - val_accuracy: 0.6267 - val_loss: 0.6319\n",
      "Epoch 3/5\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 61ms/step - accuracy: 0.7305 - loss: 0.5305 - val_accuracy: 0.6974 - val_loss: 0.5886\n",
      "Epoch 4/5\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 60ms/step - accuracy: 0.8062 - loss: 0.4217 - val_accuracy: 0.7114 - val_loss: 0.5981\n",
      "Epoch 5/5\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 61ms/step - accuracy: 0.8396 - loss: 0.3669 - val_accuracy: 0.7454 - val_loss: 0.6079\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 7ms/step - accuracy: 0.7413 - loss: 0.6145\n",
      "Test score: 0.6078584790229797\n",
      "Test accuracy: 0.7453600168228149\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers import SimpleRNN\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_features, 128))\n",
    "model.add(SimpleRNN(128, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, validation_data=(x_test, y_test))\n",
    "\n",
    "score, acc = model.evaluate(x_test, y_test, batch_size=batch_size)\n",
    "print('Test score:', score)\n",
    "print('Test accuracy:', acc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results of running the different RNN models (LSTM, GRU, SimpleRNN) on the IMDB dataset are as follows:\n",
    "\n",
    "### LSTM:\n",
    "\n",
    "Test score: 0.5174\n",
    "\n",
    "Test accuracy: 82.1%\n",
    "\n",
    "### GRU:\n",
    "\n",
    "Test score: 0.6336\n",
    "\n",
    "Test accuracy: 66.3%\n",
    "\n",
    "### SimpleRNN:\n",
    "\n",
    "Test score: 0.6079\n",
    "\n",
    "Test accuracy: 74.5%\n",
    "\n",
    "### Analysis\n",
    "#### LSTM (Long Short-Term Memory)\n",
    "\n",
    "The LSTM model outperforms both GRU and SimpleRNN in terms of test accuracy, achieving an accuracy of 82.1%.\n",
    "LSTM's ability to capture long-term dependencies makes it particularly effective for the IMDB sentiment analysis task.\n",
    "\n",
    "#### GRU (Gated Recurrent Unit)\n",
    "\n",
    "The GRU model shows a lower performance compared to LSTM, with a test accuracy of 66.3%.\n",
    "While GRUs are also designed to handle long-term dependencies, they are sometimes less powerful than LSTMs for certain tasks, though they can be more computationally efficient.\n",
    "\n",
    "#### SimpleRNN\n",
    "\n",
    "The SimpleRNN model achieves a test accuracy of 74.5%, which is better than GRU but not as good as LSTM.\n",
    "SimpleRNNs struggle with capturing long-term dependencies, which might explain why their performance is not as strong as LSTM's.\n",
    "\n",
    "#### Conclusion\n",
    "From the above results, it is evident that LSTM performs the best for the IMDB sentiment analysis task. This reinforces the understanding that LSTM networks are well-suited for tasks requiring the modeling of long-term dependencies in sequences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Problem 2] (Advance assignment) Comparison between multiple data sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/topister/Desktop/dpro/machine-learning/dpro/lib/python3.10/site-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n",
      "2024-07-31 12:35:08.995581: W external/local_tsl/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 359280000 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "\u001b[1m281/281\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - accuracy: 0.6245 - loss: 1.7499"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-31 12:42:52.557090: W external/local_tsl/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 89840000 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m281/281\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 88ms/step - accuracy: 0.6249 - loss: 1.7481 - val_accuracy: 0.8028 - val_loss: 0.8690\n",
      "Epoch 2/5\n",
      "\u001b[1m281/281\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 85ms/step - accuracy: 0.8958 - loss: 0.4847 - val_accuracy: 0.8090 - val_loss: 0.8312\n",
      "Epoch 3/5\n",
      "\u001b[1m281/281\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 84ms/step - accuracy: 0.9366 - loss: 0.2762 - val_accuracy: 0.8103 - val_loss: 0.8739\n",
      "Epoch 4/5\n",
      "\u001b[1m281/281\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 83ms/step - accuracy: 0.9554 - loss: 0.1970 - val_accuracy: 0.8117 - val_loss: 0.8999\n",
      "Epoch 5/5\n",
      "\u001b[1m281/281\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 82ms/step - accuracy: 0.9499 - loss: 0.2015 - val_accuracy: 0.8108 - val_loss: 0.9298\n",
      "\u001b[1m16/71\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8200 - loss: 0.9200"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-31 12:44:27.596081: W external/local_tsl/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 89840000 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.8168 - loss: 0.8992\n",
      "Test score: 0.9298367500305176\n",
      "Test accuracy: 0.8107746839523315\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.datasets import reuters\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.layers import Dropout\n",
    "\n",
    "max_words = 10000\n",
    "batch_size = 32\n",
    "\n",
    "# Load data\n",
    "(x_train, y_train), (x_test, y_test) = reuters.load_data(num_words=max_words)\n",
    "\n",
    "# Pad sequences\n",
    "tokenizer = Tokenizer(num_words=max_words)\n",
    "x_train = tokenizer.sequences_to_matrix(x_train, mode='binary')\n",
    "x_test = tokenizer.sequences_to_matrix(x_test, mode='binary')\n",
    "\n",
    "# Convert labels to categorical\n",
    "num_classes = np.max(y_train) + 1\n",
    "y_train = tf.keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = tf.keras.utils.to_categorical(y_test, num_classes)\n",
    "\n",
    "# Model\n",
    "model = Sequential()\n",
    "model.add(Dense(512, input_shape=(max_words,), activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, validation_data=(x_test, y_test))\n",
    "\n",
    "score, acc = model.evaluate(x_test, y_test, batch_size=batch_size)\n",
    "print('Test score:', score)\n",
    "print('Test accuracy:', acc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Problem 3] Explanation of other classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RNN - The base class for all recurrent layers.\n",
    "\n",
    "SimpleRNNCell - The cell class for SimpleRNN, representing the internal computations of SimpleRNN.\n",
    "\n",
    "GRUCell - The cell class for GRU, representing the internal computations of GRU.\n",
    "\n",
    "LSTMCell - The cell class for LSTM, representing the internal computations of LSTM.\n",
    "\n",
    "StackedRNNCells - A wrapper for stacking multiple RNN cells.\n",
    "\n",
    "CuDNNGRU - A fast GRU implementation leveraging NVIDIA's CuDNN library for GPU acceleration.\n",
    "\n",
    "CuDNNLSTM - A fast LSTM implementation leveraging NVIDIA's CuDNN library for GPU acceleration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dpro",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
