{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Problem 1] Creating a one-dimensional convolutional layer class that limits the number of channels to one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class SimpleConv1d:\n",
    "    def __init__(self, filter_size, initializer, optimizer):\n",
    "        self.filter_size = filter_size\n",
    "        self.optimizer = optimizer\n",
    "        self.W = initializer.W(1, filter_size)  # filter size for one channel\n",
    "        self.b = initializer.B(1)  # bias term\n",
    "\n",
    "    def forward(self, X):\n",
    "        self.X = X\n",
    "        self.out_size = len(X) - self.filter_size + 1\n",
    "        self.A = np.array([np.dot(self.X[i:i+self.filter_size], self.W) for i in range(self.out_size)]) + self.b\n",
    "        return self.A\n",
    "\n",
    "    def backward(self, dA):\n",
    "        self.dW = np.zeros_like(self.W, dtype=np.float64)\n",
    "        self.db = dA.sum()\n",
    "        self.dX = np.zeros_like(self.X, dtype=np.float64)\n",
    "        \n",
    "        for i in range(self.out_size):\n",
    "            self.dW += dA[i] * self.X[i:i+self.filter_size]\n",
    "            self.dX[i:i+self.filter_size] += dA[i] * self.W\n",
    "            \n",
    "        self = self.optimizer.update(self)\n",
    "        return self.dX    \n",
    "\n",
    "class SimpleInitializer:\n",
    "    def __init__(self, sigma):\n",
    "        self.sigma = sigma\n",
    "\n",
    "    def W(self, n_nodes1, n_nodes2):\n",
    "        return self.sigma * np.random.randn(n_nodes1, n_nodes2).astype(np.float64)\n",
    "\n",
    "    def B(self, n_nodes):\n",
    "        return np.zeros(n_nodes, dtype=np.float64)\n",
    "\n",
    "class SGD:\n",
    "    def __init__(self, lr):\n",
    "        self.lr = lr\n",
    "\n",
    "    def update(self, layer):\n",
    "        layer.W -= self.lr * layer.dW\n",
    "        layer.b -= self.lr * layer.db\n",
    "        return layer\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Problem 2] Output size calculation after one-dimensional convolution\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output size: 8\n"
     ]
    }
   ],
   "source": [
    "def calculate_output_size(input_size, filter_size, padding=0, stride=1):\n",
    "    return (input_size - filter_size + 2 * padding) // stride + 1\n",
    "\n",
    "# usage\n",
    "input_size = 10\n",
    "filter_size = 3\n",
    "output_size = calculate_output_size(input_size, filter_size)\n",
    "print(f'Output size: {output_size}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Problem 3] Experiment of one-dimensional convolutional layer with small array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example input, weight, and bias\n",
    "x = np.array([1, 2, 3, 4], dtype=np.float64)\n",
    "w = np.array([3, 5, 7], dtype=np.float64)\n",
    "b = np.array([1], dtype=np.float64)\n",
    "\n",
    "# Expected output\n",
    "expected_a = np.array([35, 50], dtype=np.float64)\n",
    "\n",
    "# Instantiate initializer and optimizer\n",
    "initializer = SimpleInitializer(sigma=0.01)\n",
    "optimizer = SGD(lr=0.01)\n",
    "# Create convolutional layer\n",
    "conv1d = SimpleConv1d(filter_size=3, initializer=initializer, optimizer=optimizer)\n",
    "conv1d.W = w  # Manually set weights for testing\n",
    "conv1d.b = b  # Manually set bias for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forward propagation output: [35. 50.]\n",
      "Expected output: [35. 50.]\n"
     ]
    }
   ],
   "source": [
    "# Forward propagation\n",
    "a = conv1d.forward(x)\n",
    "print(f'Forward propagation output: {a}')\n",
    "print(f'Expected output: {expected_a}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient wrt input: [ 30. 110. 170. 140.]\n"
     ]
    }
   ],
   "source": [
    "# Backward propagation\n",
    "delta_a = np.array([10, 20], dtype=np.float64)\n",
    "delta_x = conv1d.backward(delta_a)\n",
    "print(f'Gradient wrt input: {delta_x}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Problem 4] Creating a one-dimensional convolutional layer class that does not limit the number of channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class Conv1d:\n",
    "    def __init__(self, filter_size, in_channels, out_channels, initializer=None, optimizer=None):\n",
    "        self.filter_size = filter_size\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        \n",
    "        # Initialize weights and biases\n",
    "        if initializer is None:\n",
    "            self.initializer = SimpleInitializer(sigma=0.01)\n",
    "        else:\n",
    "            self.initializer = initializer\n",
    "            \n",
    "        self.W = self.initializer.W(out_channels, in_channels, filter_size)\n",
    "        self.b = np.zeros(out_channels)\n",
    "        \n",
    "        # Optimizer\n",
    "        if optimizer is None:\n",
    "            self.optimizer = SGD(lr=0.01)\n",
    "        else:\n",
    "            self.optimizer = optimizer\n",
    "        \n",
    "        # Variables to store gradients\n",
    "        self.dW = None\n",
    "        self.db = None\n",
    "        self.dX = None\n",
    "        \n",
    "        # Variables to store intermediate values during forward pass\n",
    "        self.X = None\n",
    "        self.out_size = None\n",
    "    \n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        Forward propagation through the convolutional layer.\n",
    "        \n",
    "        Parameters:\n",
    "        - X: Input data, a 2D array of shape (in_channels, input_size)\n",
    "        \n",
    "        Returns:\n",
    "        - A: Output data after convolution, a 2D array of shape (out_channels, output_size)\n",
    "        \"\"\"\n",
    "        self.X = X\n",
    "        in_channels, input_size = X.shape\n",
    "        self.out_size = input_size - self.filter_size + 1\n",
    "        A = np.zeros((self.out_channels, self.out_size))\n",
    "        \n",
    "        for i in range(self.out_channels):\n",
    "            for j in range(self.in_channels):\n",
    "                A[i] += np.convolve(X[j], self.W[i, j], mode='valid') + self.b[i]\n",
    "        \n",
    "        return A\n",
    "    \n",
    "    def backward(self, dA):\n",
    "        \"\"\"\n",
    "        Backward propagation to compute gradients.\n",
    "        \n",
    "        Parameters:\n",
    "        - dA: Gradient of loss with respect to output, a 2D array of shape (out_channels, output_size)\n",
    "        \n",
    "        Returns:\n",
    "        - dX: Gradient of loss with respect to input, a 2D array of shape (in_channels, input_size)\n",
    "        \"\"\"\n",
    "        in_channels, input_size = self.X.shape\n",
    "        self.dW = np.zeros_like(self.W)\n",
    "        self.db = np.zeros_like(self.b)\n",
    "        self.dX = np.zeros_like(self.X)\n",
    "        \n",
    "        for i in range(self.out_channels):\n",
    "            for j in range(self.in_channels):\n",
    "                self.dW[i, j] += np.convolve(self.X[j], dA[i], mode='valid')\n",
    "                self.db[i] += np.sum(dA[i])\n",
    "                self.dX[j] += np.convolve(dA[i], self.W[i, j][::-1], mode='full')  # Reverse W for convolution\n",
    "            \n",
    "        self = self.optimizer.update(self)\n",
    "        return self.dX\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Problem 5] (Advanced task) Implementing padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conv1d:\n",
    "    def __init__(self, filter_size, in_channels, out_channels, initializer=None, optimizer=None, padding=0):\n",
    "        self.filter_size = filter_size\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.padding = padding\n",
    "        \n",
    "        # Initialize weights and biases\n",
    "        if initializer is None:\n",
    "            self.initializer = SimpleInitializer(sigma=0.01)\n",
    "        else:\n",
    "            self.initializer = initializer\n",
    "            \n",
    "        self.W = self.initializer.W(out_channels, in_channels, filter_size)\n",
    "        self.b = np.zeros(out_channels)\n",
    "        \n",
    "        # Optimizer\n",
    "        if optimizer is None:\n",
    "            self.optimizer = SGD(lr=0.01)\n",
    "        else:\n",
    "            self.optimizer = optimizer\n",
    "        \n",
    "        # Variables to store gradients\n",
    "        self.dW = None\n",
    "        self.db = None\n",
    "        self.dX = None\n",
    "        \n",
    "        # Variables to store intermediate values during forward pass\n",
    "        self.X = None\n",
    "        self.out_size = None\n",
    "    \n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        Forward propagation through the convolutional layer.\n",
    "        \n",
    "        Parameters:\n",
    "        - X: Input data, a 2D array of shape (in_channels, input_size)\n",
    "        \n",
    "        Returns:\n",
    "        - A: Output data after convolution, a 2D array of shape (out_channels, output_size)\n",
    "        \"\"\"\n",
    "        self.X = X\n",
    "        in_channels, input_size = X.shape\n",
    "        self.out_size = input_size - self.filter_size + 1 + 2 * self.padding\n",
    "        A = np.zeros((self.out_channels, self.out_size))\n",
    "        \n",
    "        padded_X = np.pad(X, ((0, 0), (self.padding, self.padding)), mode='constant')\n",
    "        \n",
    "        for i in range(self.out_channels):\n",
    "            for j in range(self.in_channels):\n",
    "                A[i] += np.convolve(padded_X[j], self.W[i, j], mode='valid') + self.b[i]\n",
    "        \n",
    "        return A\n",
    "    \n",
    "    def backward(self, dA):\n",
    "        \"\"\"\n",
    "        Backward propagation to compute gradients.\n",
    "        \n",
    "        Parameters:\n",
    "        - dA: Gradient of loss with respect to output, a 2D array of shape (out_channels, output_size)\n",
    "        \n",
    "        Returns:\n",
    "        - dX: Gradient of loss with respect to input, a 2D array of shape (in_channels, input_size)\n",
    "        \"\"\"\n",
    "        in_channels, input_size = self.X.shape\n",
    "        padded_X = np.pad(self.X, ((0, 0), (self.padding, self.padding)), mode='constant')\n",
    "        self.dW = np.zeros_like(self.W)\n",
    "        self.db = np.zeros_like(self.b)\n",
    "        self.dX = np.zeros_like(padded_X)\n",
    "        \n",
    "        for i in range(self.out_channels):\n",
    "            for j in range(self.in_channels):\n",
    "                self.dW[i, j] += np.convolve(padded_X[j], dA[i], mode='valid')\n",
    "                self.db[i] += np.sum(dA[i])\n",
    "                self.dX[j] += np.convolve(dA[i], self.W[i, j][::-1], mode='full')  # Reverse W for convolution\n",
    "        \n",
    "        # Trim dX to remove padded values\n",
    "        dX = self.dX[:, self.padding:self.padding + input_size]\n",
    "        \n",
    "        self = self.optimizer.update(self)\n",
    "        return dX\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Problem 6] (Advanced task) Response to mini batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conv1d:\n",
    "    def __init__(self, filter_size, in_channels, out_channels, initializer=None, optimizer=None, padding=0, batch_size=1):\n",
    "        self.filter_size = filter_size\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.padding = padding\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        # Initialize weights and biases\n",
    "        if initializer is None:\n",
    "            self.initializer = SimpleInitializer(sigma=0.01)\n",
    "        else:\n",
    "            self.initializer = initializer\n",
    "            \n",
    "        self.W = self.initializer.W(out_channels, in_channels, filter_size)\n",
    "        self.b = np.zeros(out_channels)\n",
    "        \n",
    "        # Optimizer\n",
    "        if optimizer is None:\n",
    "            self.optimizer = SGD(lr=0.01)\n",
    "        else:\n",
    "            self.optimizer = optimizer\n",
    "        \n",
    "        # Variables to store gradients\n",
    "        self.dW = None\n",
    "        self.db = None\n",
    "        self.dX = None\n",
    "        \n",
    "        # Variables to store intermediate values during forward pass\n",
    "        self.X = None\n",
    "        self.out_size = None\n",
    "    \n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        Forward propagation through the convolutional layer.\n",
    "        \n",
    "        Parameters:\n",
    "        - X: Input data, a 3D array of shape (batch_size, in_channels, input_size)\n",
    "        \n",
    "        Returns:\n",
    "        - A: Output data after convolution, a 3D array of shape (batch_size, out_channels, output_size)\n",
    "        \"\"\"\n",
    "        self.X = X\n",
    "        batch_size, in_channels, input_size = X.shape\n",
    "        self.out_size = input_size - self.filter_size + 1 + 2 * self.padding\n",
    "        A = np.zeros((batch_size, self.out_channels, self.out_size))\n",
    "        \n",
    "        padded_X = np.pad(X, ((0, 0), (0, 0), (self.padding, self.padding)), mode='constant')\n",
    "        \n",
    "        for b in range(batch_size):\n",
    "            for i in range(self.out_channels):\n",
    "                for j in range(self.in_channels):\n",
    "                    A[b, i] += np.convolve(padded_X[b, j], self.W[i, j], mode='valid') + self.b[i]\n",
    "        \n",
    "        return A\n",
    "    \n",
    "    def backward(self, dA):\n",
    "        \"\"\"\n",
    "        Backward propagation to compute gradients.\n",
    "        \n",
    "        Parameters:\n",
    "        - dA: Gradient of loss with respect to output, a 3D array of shape (batch_size, out_channels, output_size)\n",
    "        \n",
    "        Returns:\n",
    "        - dX: Gradient of loss with respect to input, a 3D array of shape (batch_size, in_channels, input_size)\n",
    "        \"\"\"\n",
    "        batch_size, in_channels, input_size = self.X.shape\n",
    "        padded_X = np.pad(self.X, ((0, 0), (0, 0), (self.padding, self.padding)), mode='constant')\n",
    "        self.dW = np.zeros_like(self.W)\n",
    "        self.db = np.zeros_like(self.b)\n",
    "        self.dX = np.zeros_like(padded_X)\n",
    "        \n",
    "        for b in range(batch_size):\n",
    "            for i in range(self.out_channels):\n",
    "                for j in range(self.in_channels):\n",
    "                    self.dW[i, j] += np.convolve(padded_X[b, j], dA[b, i], mode='valid')\n",
    "                    self.db[i] += np.sum(dA[b, i])\n",
    "                    self.dX[b, j] += np.convolve(dA[b, i], self.W[i, j][::-1], mode='full')  # Reverse W for convolution\n",
    "        \n",
    "        # Trim dX to remove padded values\n",
    "        dX = self.dX[:, :, self.padding:self.padding + input_size]\n",
    "        \n",
    "        self = self.optimizer.update(self)\n",
    "        return dX\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Problem 7] (Advance assignment) Arbitrary number of strides"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conv1d:\n",
    "    def __init__(self, filter_size, in_channels, out_channels, initializer=None, optimizer=None, padding=0, batch_size=1, stride=1):\n",
    "        self.filter_size = filter_size\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.padding = padding\n",
    "        self.batch_size = batch_size\n",
    "        self.stride = stride\n",
    "        \n",
    "        # Initialize weights and biases\n",
    "        if initializer is None:\n",
    "            self.initializer = SimpleInitializer(sigma=0.01)\n",
    "        else:\n",
    "            self.initializer = initializer\n",
    "            \n",
    "        self.W = self.initializer.W(out_channels, in_channels, filter_size)\n",
    "        self.b = np.zeros(out_channels)\n",
    "        \n",
    "        # Optimizer\n",
    "        if optimizer is None:\n",
    "            self.optimizer = SGD(lr=0.01)\n",
    "        else:\n",
    "            self.optimizer = optimizer\n",
    "        \n",
    "        # Variables to store gradients\n",
    "        self.dW = None\n",
    "        self.db = None\n",
    "        self.dX = None\n",
    "        \n",
    "        # Variables to store intermediate values during forward pass\n",
    "        self.X = None\n",
    "        self.out_size = None\n",
    "    \n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        Forward propagation through the convolutional layer.\n",
    "        \n",
    "        Parameters:\n",
    "        - X: Input data, a 3D array of shape (batch_size, in_channels, input_size)\n",
    "        \n",
    "        Returns:\n",
    "        - A: Output data after convolution, a 3D array of shape (batch_size, out_channels, output_size)\n",
    "        \"\"\"\n",
    "        self.X = X\n",
    "        batch_size, in_channels, input_size = X.shape\n",
    "        self.out_size = (input_size - self.filter_size + 2 * self.padding) // self.stride + 1\n",
    "        A = np.zeros((batch_size, self.out_channels, self.out_size))\n",
    "        \n",
    "        padded_X = np.pad(X, ((0, 0), (0, 0), (self.padding, self.padding)), mode='constant')\n",
    "        \n",
    "        for b in range(batch_size):\n",
    "            for i in range(self.out_channels):\n",
    "                for j in range(self.in_channels):\n",
    "                    for k in range(self.out_size):\n",
    "                        start = k * self.stride\n",
    "                        end = start + self.filter_size\n",
    "                        A[b, i, k] += np.sum(padded_X[b, j, start:end] * self.W[i, j]) + self.b[i]\n",
    "        \n",
    "        return A\n",
    "    \n",
    "    def backward(self, dA):\n",
    "        \"\"\"\n",
    "        Backward propagation to compute gradients.\n",
    "        \n",
    "        Parameters:\n",
    "        - dA: Gradient of loss with respect to output, a 3D array of shape (batch_size, out_channels, output_size)\n",
    "        \n",
    "        Returns:\n",
    "        - dX: Gradient of loss with respect to input, a 3D array of shape (batch_size, in_channels, input_size)\n",
    "        \"\"\"\n",
    "        batch_size, in_channels, input_size = self.X.shape\n",
    "        padded_X = np.pad(self.X, ((0, 0), (0, 0), (self.padding, self.padding)), mode='constant')\n",
    "        self.dW = np.zeros_like(self.W)\n",
    "        self.db = np.zeros_like(self.b)\n",
    "        self.dX = np.zeros_like(padded_X)\n",
    "        \n",
    "        for b in range(batch_size):\n",
    "            for i in range(self.out_channels):\n",
    "                for j in range(self.in_channels):\n",
    "                    for k in range(self.out_size):\n",
    "                        start = k * self.stride\n",
    "                        end = start + self.filter_size\n",
    "                        self.dW[i, j] += np.dot(dA[b, i, k], padded_X[b, j, start:end])\n",
    "                        self.db[i] += dA[b, i, k]\n",
    "                        self.dX[b, j, start:end] += dA[b, i, k] * self.W[i, j]\n",
    "        \n",
    "        # Trim dX to remove padded values\n",
    "        dX = self.dX[:, :, self.padding:self.padding + input_size]\n",
    "        \n",
    "        self = self.optimizer.update(self)\n",
    "        return dX\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Problem 8] Learning and estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaxPool1d:\n",
    "    def __init__(self, pool_size=2, stride=2):\n",
    "        self.pool_size = pool_size\n",
    "        self.stride = stride\n",
    "        self.X = None\n",
    "        self.arg_max_indices = None\n",
    "    \n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        Forward propagation through the Max Pooling layer.\n",
    "        \n",
    "        Parameters:\n",
    "        - X: Input data, a 3D array of shape (batch_size, channels, input_size)\n",
    "        \n",
    "        Returns:\n",
    "        - A: Output data after pooling, a 3D array of shape (batch_size, channels, output_size)\n",
    "        \"\"\"\n",
    "        self.X = X\n",
    "        batch_size, channels, input_size = X.shape\n",
    "        output_size = (input_size - self.pool_size) // self.stride + 1\n",
    "        A = np.zeros((batch_size, channels, output_size))\n",
    "        self.arg_max_indices = np.zeros((batch_size, channels, output_size), dtype=np.int32)\n",
    "        \n",
    "        for b in range(batch_size):\n",
    "            for c in range(channels):\n",
    "                for k in range(output_size):\n",
    "                    start = k * self.stride\n",
    "                    end = start + self.pool_size\n",
    "                    A[b, c, k] = np.max(X[b, c, start:end])\n",
    "                    self.arg_max_indices[b, c, k] = np.argmax(X[b, c, start:end]) + start\n",
    "        \n",
    "        return A\n",
    "    \n",
    "    def backward(self, dA):\n",
    "        \"\"\"\n",
    "        Backward propagation through the Max Pooling layer (gradient is passed through max values).\n",
    "        \n",
    "        Parameters:\n",
    "        - dA: Gradient of loss with respect to output of Max Pooling layer, a 3D array of shape (batch_size, channels, output_size)\n",
    "        \n",
    "        Returns:\n",
    "        - dX: Gradient of loss with respect to input of Max Pooling layer, a 3D array of shape (batch_size, channels, input_size)\n",
    "        \"\"\"\n",
    "        batch_size, channels, output_size = dA.shape\n",
    "        dX = np.zeros_like(self.X)\n",
    "        \n",
    "        for b in range(batch_size):\n",
    "            for c in range(channels):\n",
    "                for k in range(output_size):\n",
    "                    idx = self.arg_max_indices[b, c, k]\n",
    "                    dX[b, c, idx] += dA[b, c, k]\n",
    "        \n",
    "        return dX\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.datasets import mnist\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load MNIST data\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "\n",
    "# Preprocess data\n",
    "X_train = X_train.astype('float32') / 255.0\n",
    "X_test = X_test.astype('float32') / 255.0\n",
    "X_train = np.expand_dims(X_train, axis=-1)  # Add channel dimension\n",
    "X_test = np.expand_dims(X_test, axis=-1)    # Add channel dimension\n",
    "y_train = to_categorical(y_train, 10)\n",
    "y_test = to_categorical(y_test, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=(28, 28, 1)))  # Initial convolutional layer\n",
    "model.add(Conv2D(64, (3, 3), activation='relu'))  # Additional convolutional layers\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dense(10, activation='softmax'))  # Output layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile model\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-02 11:17:16.694517: W external/local_tsl/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 169344000 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 139ms/step - accuracy: 0.8770 - loss: 0.3909 - val_accuracy: 0.9848 - val_loss: 0.0553\n",
      "Epoch 2/10\n",
      "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 138ms/step - accuracy: 0.9844 - loss: 0.0512 - val_accuracy: 0.9855 - val_loss: 0.0542\n",
      "Epoch 3/10\n",
      "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 139ms/step - accuracy: 0.9912 - loss: 0.0282 - val_accuracy: 0.9882 - val_loss: 0.0502\n",
      "Epoch 4/10\n",
      "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 139ms/step - accuracy: 0.9935 - loss: 0.0207 - val_accuracy: 0.9873 - val_loss: 0.0500\n",
      "Epoch 5/10\n",
      "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 140ms/step - accuracy: 0.9957 - loss: 0.0123 - val_accuracy: 0.9875 - val_loss: 0.0526\n",
      "Epoch 6/10\n",
      "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 142ms/step - accuracy: 0.9974 - loss: 0.0086 - val_accuracy: 0.9888 - val_loss: 0.0591\n",
      "Epoch 7/10\n",
      "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 144ms/step - accuracy: 0.9972 - loss: 0.0092 - val_accuracy: 0.9873 - val_loss: 0.0548\n",
      "Epoch 8/10\n",
      "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 143ms/step - accuracy: 0.9979 - loss: 0.0058 - val_accuracy: 0.9897 - val_loss: 0.0622\n",
      "Epoch 9/10\n",
      "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 143ms/step - accuracy: 0.9978 - loss: 0.0063 - val_accuracy: 0.9872 - val_loss: 0.0659\n",
      "Epoch 10/10\n",
      "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 143ms/step - accuracy: 0.9984 - loss: 0.0054 - val_accuracy: 0.9900 - val_loss: 0.0539\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x7d2e5c42b880>"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the model\n",
    "model.fit(X_train, y_train, batch_size=128, epochs=10, validation_split=0.1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - accuracy: 0.9885 - loss: 0.0454\n",
      "Accuracy on test set: 0.9904\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model\n",
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "print(f'Accuracy on test set: {accuracy:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dpro",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
