{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Problem 1] Fully Connected Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class FC:\n",
    "    \"\"\"\n",
    "    Fully connected layer\n",
    "    Parameters\n",
    "    ----------\n",
    "    n_nodes1 : int\n",
    "      Number of nodes in the previous layer\n",
    "    n_nodes2 : int\n",
    "      Number of nodes in the next layer\n",
    "    initializer: instance of initialization method\n",
    "    optimizer: instance of optimization method\n",
    "    \"\"\"\n",
    "    def __init__(self, n_nodes1, n_nodes2, initializer, optimizer):\n",
    "        self.optimizer = optimizer\n",
    "        self.W = initializer.W(n_nodes1, n_nodes2)\n",
    "        self.B = initializer.B(n_nodes2)\n",
    "    \n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        Forward propagation\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : ndarray, shape (batch_size, n_nodes1)\n",
    "            Input\n",
    "        Returns\n",
    "        ----------\n",
    "        A : ndarray, shape (batch_size, n_nodes2)\n",
    "            Output\n",
    "        \"\"\"  \n",
    "\n",
    "        self.X = X\n",
    "        return np.dot(X, self.W) + self.B      \n",
    "    \n",
    "    def backward(self, dA):\n",
    "        \"\"\"\n",
    "        Backward propagation\n",
    "        Parameters\n",
    "        ----------\n",
    "        dA : ndarray, shape (batch_size, n_nodes2)\n",
    "            Gradient from the next layer\n",
    "        Returns\n",
    "        ----------\n",
    "        dZ : ndarray, shape (batch_size, n_nodes1)\n",
    "            Gradient to be passed to the previous layer\n",
    "        \"\"\"\n",
    "    \n",
    "\n",
    "        self.dW = np.dot(self.X.T, dA)\n",
    "        self.dB = np.sum(dA, axis=0)\n",
    "        dX = np.dot(dA, self.W.T)\n",
    "        self = self.optimizer.update(self)\n",
    "        return dX\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Problem 2] Initialization Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleInitializer:\n",
    "    \"\"\"\n",
    "    Simple initialization with Gaussian distribution\n",
    "    Parameters\n",
    "    ----------\n",
    "    sigma : float\n",
    "      Standard deviation of Gaussian distribution\n",
    "    \"\"\"\n",
    "    def __init__(self, sigma):\n",
    "        self.sigma = sigma\n",
    "    \n",
    "    def W(self, n_nodes1, n_nodes2):\n",
    "        \"\"\"\n",
    "        Weight initialization\n",
    "        Parameters\n",
    "        ----------\n",
    "        n_nodes1 : int\n",
    "          Number of nodes in the previous layer\n",
    "        n_nodes2 : int\n",
    "          Number of nodes in the next layer\n",
    "        Returns\n",
    "        ----------\n",
    "        W : ndarray, shape (n_nodes1, n_nodes2)\n",
    "            Initialized weights\n",
    "        \"\"\"\n",
    "        return self.sigma * np.random.randn(n_nodes1, n_nodes2)\n",
    "    \n",
    "    def B(self, n_nodes2):\n",
    "        \"\"\"\n",
    "        Bias initialization\n",
    "        Parameters\n",
    "        ----------\n",
    "        n_nodes2 : int\n",
    "          Number of nodes in the next layer\n",
    "        Returns\n",
    "        ----------\n",
    "        B : ndarray, shape (n_nodes2,)\n",
    "            Initialized biases\n",
    "        \"\"\"\n",
    "        return np.zeros(n_nodes2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Problem 3] Optimization Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SGD:\n",
    "    \"\"\"\n",
    "    Stochastic gradient descent\n",
    "    Parameters\n",
    "    ----------\n",
    "    lr : float\n",
    "      Learning rate\n",
    "    \"\"\"\n",
    "    def __init__(self, lr):\n",
    "        self.lr = lr\n",
    "    \n",
    "    def update(self, layer):\n",
    "        \"\"\"\n",
    "        Update weights and biases for a layer\n",
    "        Parameters\n",
    "        ----------\n",
    "        layer : Instance of the layer before update\n",
    "        \"\"\"\n",
    "        layer.W -= self.lr * layer.dW\n",
    "        layer.B -= self.lr * layer.dB\n",
    "        return layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Problem 4] Activation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tanh:\n",
    "    def forward(self, A):\n",
    "        self.Z = np.tanh(A)\n",
    "        return self.Z\n",
    "\n",
    "    def backward(self, dZ):\n",
    "        return dZ * (1 - self.Z ** 2)\n",
    "\n",
    "class Softmax:\n",
    "    def forward(self, A):\n",
    "        expA = np.exp(A - np.max(A, axis=1, keepdims=True))\n",
    "        self.Z = expA / np.sum(expA, axis=1, keepdims=True)\n",
    "        return self.Z\n",
    "\n",
    "    def backward(self, Z, Y):\n",
    "        return self.Z - Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Problem 5] ReLU Activation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReLU:\n",
    "    def forward(self, A):\n",
    "        self.A = A\n",
    "        return np.maximum(0, A)\n",
    "    \n",
    "    def backward(self, dZ):\n",
    "        return dZ * (self.A > 0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Problem 6] Weight Initialization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class XavierInitializer:\n",
    "    def W(self, n_nodes1, n_nodes2):\n",
    "        return np.random.randn(n_nodes1, n_nodes2) / np.sqrt(n_nodes1)\n",
    "    \n",
    "    def B(self, n_nodes2):\n",
    "        return np.zeros(n_nodes2)\n",
    "\n",
    "class HeInitializer:\n",
    "    def W(self, n_nodes1, n_nodes2):\n",
    "        return np.random.randn(n_nodes1, n_nodes2) * np.sqrt(2 / n_nodes1)\n",
    "    \n",
    "    def B(self, n_nodes2):\n",
    "        return np.zeros(n_nodes2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Problem 7] AdaGrad Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdaGrad:\n",
    "    def __init__(self, lr):\n",
    "        self.lr = lr\n",
    "        self.HW = 0\n",
    "        self.HB = 0\n",
    "    \n",
    "    def update(self, layer):\n",
    "        self.HW += layer.dW * layer.dW\n",
    "        self.HB += layer.dB * layer.dB\n",
    "        layer.W -= self.lr * layer.dW / (np.sqrt(self.HW) + 1e-7)\n",
    "        layer.B -= self.lr * layer.dB / (np.sqrt(self.HB) + 1e-7)\n",
    "        return layer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Problem 8] Scratch Deep Neural Network Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "class ScratchDeepNeuralNetworkClassifier:\n",
    "    def __init__(self, n_features, n_output, sigma, lr, n_nodes1, n_nodes2, initializer='simple', optimizer='sgd'):\n",
    "        self.n_features = n_features\n",
    "        self.n_output = n_output\n",
    "        self.sigma = sigma\n",
    "        self.lr = lr\n",
    "        self.n_nodes1 = n_nodes1\n",
    "        self.n_nodes2 = n_nodes2\n",
    "\n",
    "        if initializer == 'simple':\n",
    "            self.initializer = SimpleInitializer(self.sigma)\n",
    "        if optimizer == 'sgd':\n",
    "            self.optimizer = SGD(self.lr)\n",
    "\n",
    "        self.FC1 = FC(self.n_features, self.n_nodes1, self.initializer, self.optimizer)\n",
    "        self.activation1 = Tanh()\n",
    "        self.FC2 = FC(self.n_nodes1, self.n_nodes2, self.initializer, self.optimizer)\n",
    "        self.activation2 = Tanh()\n",
    "        self.FC3 = FC(self.n_nodes2, self.n_output, self.initializer, self.optimizer)\n",
    "        self.activation3 = Softmax()\n",
    "\n",
    "    def fit(self, X, y, epochs, batch_size):\n",
    "        n_samples = X.shape[0]\n",
    "        for epoch in range(epochs):\n",
    "            for i in range(0, n_samples, batch_size):\n",
    "                X_batch = X[i:i+batch_size]\n",
    "                y_batch = y[i:i+batch_size]\n",
    "\n",
    "                # Forward\n",
    "                A1 = self.FC1.forward(X_batch)\n",
    "                Z1 = self.activation1.forward(A1)\n",
    "                A2 = self.FC2.forward(Z1)\n",
    "                Z2 = self.activation2.forward(A2)\n",
    "                A3 = self.FC3.forward(Z2)\n",
    "                Z3 = self.activation3.forward(A3)\n",
    "\n",
    "                # Backward\n",
    "                dA3 = self.activation3.backward(Z3, y_batch)\n",
    "                dZ2 = self.FC3.backward(dA3)\n",
    "                dA2 = self.activation2.backward(dZ2)\n",
    "                dZ1 = self.FC2.backward(dA2)\n",
    "                dA1 = self.activation1.backward(dZ1)\n",
    "                dZ0 = self.FC1.backward(dA1)\n",
    "\n",
    "    def predict(self, X):\n",
    "        A1 = self.FC1.forward(X)\n",
    "        Z1 = self.activation1.forward(A1)\n",
    "        A2 = self.FC2.forward(Z1)\n",
    "        Z2 = self.activation2.forward(A2)\n",
    "        A3 = self.FC3.forward(Z2)\n",
    "        Z3 = self.activation3.forward(A3)\n",
    "        return np.argmax(Z3, axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Problem 9] Learning and Estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "mnist = fetch_openml('mnist_784')\n",
    "X = mnist.data / 255.0\n",
    "y = mnist.target.astype(int)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Convert y_train and y_test to NumPy arrays and reshape them\n",
    "y_train = y_train.to_numpy().reshape(-1, 1)\n",
    "y_test = y_test.to_numpy().reshape(-1, 1)\n",
    "\n",
    "# One-hot encode the labels\n",
    "encoder = OneHotEncoder(sparse_output=False)\n",
    "y_train_onehot = encoder.fit_transform(y_train)\n",
    "y_test_onehot = encoder.transform(y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9687142857142857\n"
     ]
    }
   ],
   "source": [
    "# Instantiate and train the model\n",
    "model = ScratchDeepNeuralNetworkClassifier(n_features=784, n_output=10, sigma=0.01, lr=0.01, n_nodes1=100, n_nodes2=50, initializer='simple', optimizer='sgd')\n",
    "model.fit(X_train, y_train_onehot, epochs=10, batch_size=32)\n",
    "\n",
    "# Predict and calculate accuracy\n",
    "y_pred = model.predict(X_test)\n",
    "y_test_labels = np.argmax(y_test_onehot, axis=1)\n",
    "accuracy = np.mean(y_pred == y_test_labels)\n",
    "print(f'Accuracy: {accuracy}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dpro",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
